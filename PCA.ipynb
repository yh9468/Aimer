{"cells":[{"cell_type":"markdown","metadata":{"id":"bM-vQh3PCP4S"},"source":["## 1.3 라이브러리 로드"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"tXWzz_ia-iKJ"},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import mean_squared_error\n","from sklearn.decomposition import PCA\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LassoLars, OrthogonalMatchingPursuit, BayesianRidge, ARDRegression\n","from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n","\n","import xgboost as xgb\n","import lightgbm as lgb\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","from catboost import CatBoostRegressor\n","\n","import optuna \n","from optuna import Trial, visualization\n","\n","import joblib\n","\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"vc1EeKkIBpCu"},"source":["## 1.4 시드 고정"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"c1uA7JH5do9P"},"outputs":[],"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","seed_everything(42) \n","SEED = 42"]},{"cell_type":"markdown","metadata":{"id":"HW6k0i0tkKTw"},"source":["# 2 데이터 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bwJWS0SFdksb"},"outputs":[],"source":["# 데이터 로드\n","train = pd.read_csv('dataset/' + 'train.csv')\n","\n","# X Y 데이터 분리\n","X_train = train.filter(regex='X') # Input : X Featrue\n","Y_train = train.filter(regex='Y') # Output : Y Feature"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KjqEH6CKsQKs"},"outputs":[],"source":["# 결과에 영향 낮은 인자 제거\n","X_train = X_train.drop(['X_04', 'X_23', 'X_47', 'X_48', 'X_10', 'X_11', 'X_02'], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zg6yTdeqFSZ2"},"outputs":[],"source":["# X_33 이상치 제거\n","\n","drop_idx = X_train.loc[X_train['X_33'] > 6 ].index\n","\n","X_train = X_train.drop(drop_idx, axis = 0)\n","Y_train = Y_train.drop(drop_idx, axis = 0)\n","\n","X_train = X_train.reset_index(drop = True)\n","Y_train = Y_train.reset_index(drop = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Gx6b3dUFY4C"},"outputs":[],"source":["# PCA 클래스 설정\n","class PCA_transform:\n","\n","  def __init__(self):\n","    self.cols_list = []\n","    self.pca_list = []\n","    self.n_pca_list = []\n","    self.size = 0\n","  \n","  # PCA 클래스의 학습 및 input 값 변환\n","  def fit_transform(self, X_input, col, n_pca):\n","\n","    X_pca = X_input[col]\n","\n","    # n차원으로 차원 축소, target 정보는 제외\n","    pca = PCA(n_components = n_pca)\n","\n","    # PCA 학습\n","    pca.fit(X_pca)\n","\n","    # PCA transform 후 데이터프레임으로 자료형 변경\n","    X_pca = pca.transform(X_pca)\n","    X_pca = pd.DataFrame(X_pca, columns = self.naming(n_pca))\n","\n","    X_input = pd.concat([X_input, X_pca], axis = 1)\n","    X_input = X_input.drop(col, axis = 1)\n","\n","    self.cols_list.append(col)\n","    self.pca_list.append(pca)\n","    self.n_pca_list.append(n_pca)\n","    self.size += 1\n","\n","    return X_input\n","\n","  # 학습된 PCA 값으로 transform\n","  def transform(self, X_input):\n","    for idx in range(self.size):\n","      X_input = self._idx_transform(X_input, idx)\n","    \n","    return X_input\n","\n","  # n번째 PCA 변환\n","  def _idx_transform(self, X_input, idx):\n","    X_pca = X_input[self.cols_list[idx]]\n","\n","    # pca transform 후 데이터프레임으로 자료형 변경\n","    X_pca = self.pca_list[idx].transform(X_pca)\n","    X_pca = pd.DataFrame(X_pca, columns = self.naming(self.n_pca_list[idx], idx))\n","\n","    X_input = pd.concat([X_input, X_pca], axis = 1)\n","    X_input = X_input.drop(self.cols_list[idx], axis = 1)\n","\n","    return X_input\n","\n","  # PCA 된 컬럼 이름 규칙\n","  def naming(self, number, name = None):\n","    if (name is None):\n","      name = self.size\n","    names = []\n","    for idx in range(number):\n","      names.append(f'PCA_{str(name)}_{idx}')\n","    return names"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mydg6d_HFl68"},"outputs":[],"source":["# Optuna로 최적화된 PCA 파라미터 적용\n","pca_5 = PCA_transform()\n","X_train = pca_5.fit_transform(X_train, ['X_13', 'X_14', 'X_15', 'X_16', 'X_17', 'X_18'], 5)\n","X_train = pca_5.fit_transform(X_train, ['X_19', 'X_20', 'X_21', 'X_22'], 2)\n","X_train = pca_5.fit_transform(X_train, ['X_34', 'X_35', 'X_36', 'X_37'], 1)\n","X_train = pca_5.fit_transform(X_train, ['X_41', 'X_42', 'X_43', 'X_44', 'X_45'], 1)\n","X_train = pca_5.fit_transform(X_train, ['X_50', 'X_51', 'X_52', 'X_53', 'X_54', 'X_55', 'X_56'], 2)"]},{"cell_type":"markdown","metadata":{"id":"T5gJCBmGn0hx"},"source":["# 3 평가산식 정의"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WPT1SxPZnvhQ"},"outputs":[],"source":["def nrmse(y_val, y_pred):\n","  rmse = mean_squared_error(y_val, y_pred, squared=False)\n","  nrmse = rmse/np.mean(np.abs(y_val))\n","  return nrmse\n","\n","def lg_nrmse(y_val, y_pred):\n","    # 각 Y Feature별 NRMSE 총합\n","    # Y_01 ~ Y_08 까지 20% 가중치 부여\n","\n","    y_val = pd.DataFrame(y_val)\n","    y_pred = pd.DataFrame(y_pred)\n","\n","    all_nrmse = []\n","    for idx in range(0,14):\n","        all_nrmse.append(nrmse(y_val.iloc[:,idx], y_pred.iloc[:,idx]))\n","        \n","    score = 1.2 * np.sum(all_nrmse[:7]) + 1.0 * np.sum(all_nrmse[7:14])\n","    return score"]},{"cell_type":"markdown","metadata":{"id":"mYrK3lm9B9GV"},"source":["# 4 모델링"]},{"cell_type":"markdown","metadata":{"id":"gQgOXbLlVXc_"},"source":["## 4.1 Cross-validation을 통해 학습하는 ML class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b00vOTUbUfu9"},"outputs":[],"source":["class CV_ml_model:\n","  # 머신러닝 모델, X_train, Y_train을 인자로 받음\n","  def __init__(self, model, X_train, Y_train):\n","    self.model = model\n","    self.name = model().__class__.__name__\n","    self.train_preds = [None] * 14\n","    self.trained_models = [None] * 14\n","    self.test_preds = [None] * 14\n","    self.X_train = self.to_np(X_train)\n","    self.Y_train = self.to_np(Y_train)\n","\n","\n","  # 넘파이 변환\n","  def to_np(self, input):\n","    if (type(input) == pd.core.frame.DataFrame):\n","      return input.to_numpy()\n","    return input\n","\n","\n","  # 이름 재설정\n","  def set_name(self, name):\n","    self.name = name\n","\n","  \n","  # Y 하나에 대해 머신러닝 수행\n","  def y_fit(self, n_folds, y_idx, X_train = None, param = {}, save = False):\n","    # X_train 따로 설정하지 않을 시 최초 입력 데이터 적용\n","    if (X_train is None):\n","      X_train = self.X_train\n","\n","    # 이미 학습된 y_idx일시 학습 중지\n","    if (self.train_preds[y_idx] is not None):\n","      return\n","\n","    # n_folds 숫자 만큼 학습된 모델이 들어갈 리스트 생성\n","    trained_y_models = [None] * n_folds\n","\n","    # 예측 결과를 넣을 ndarray 생성\n","    train_fold_pred = np.zeros((X_train.shape[0], 1))\n","\n","    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n","\n","    for folder_counter , (train_index, valid_index) in enumerate(kf.split(X_train)):\n","      #입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 셋 추출 \n","      X_tr = X_train[train_index]\n","      y_tr = self.Y_train[:,y_idx][train_index]\n","      X_val = X_train[valid_index]\n","\n","      # 폴드 세트 내부에서 만들어진 학습 데이터로 기반 모델의 학습 수행.\n","      trained_model = self.model(**param).fit(X_tr , y_tr)  \n","      # 폴드 세트 내부에서 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장.\n","      train_fold_pred[valid_index, :] = trained_model.predict(X_val).reshape(-1,1)\n","      # 학습 완료된 모델 리스트 내에 저장\n","      trained_y_models[folder_counter] = trained_model\n","    \n","    # 예측 결과 class 내 저장\n","    self.train_preds[y_idx] = train_fold_pred\n","    # 학습 완료된 모델 class 내 저장\n","    self.trained_models[y_idx] = trained_y_models\n","\n","    # 학습 완료시 모델을 파일로 저장\n","    if (save):\n","      self.save()\n","\n","  \n","  # 모델 경량화\n","  def slim_y_fit(self, n_folds, y_idx, X_train = None, param = {}, save = False):\n","    # X_train 따로 설정하지 않을 시 최초 입력 데이터 적용\n","    if (X_train is None):\n","      X_train = self.X_train\n","\n","    # 이미 학습된 y_idx일시 학습 중지\n","    if (self.train_preds[y_idx] is not None):\n","      return\n","\n","    # 예측 결과를 넣을 ndarray 생성\n","    train_fold_pred = np.zeros((X_train.shape[0], 1))\n","\n","    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n","\n","    for folder_counter , (train_index, valid_index) in enumerate(kf.split(X_train)):\n","      #입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 셋 추출 \n","      X_tr = X_train[train_index] \n","      y_tr = self.Y_train[:,y_idx][train_index] \n","      X_val = X_train[valid_index]  \n","\n","      #폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행.\n","      trained_model = self.model(**param).fit(X_tr , y_tr)  \n","      #폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장.\n","      train_fold_pred[valid_index, :] = trained_model.predict(X_val).reshape(-1,1)\n","    \n","    # 예측 결과 class 내 저장\n","    self.train_preds[y_idx] = train_fold_pred\n","    # 학습 데이터 전체로 모델 학습 후 학습 완료된 모델 class 내 저장\n","    self.trained_models[y_idx] = [self.model(**param).fit(X_train , self.Y_train[:,y_idx])]\n","\n","    # 학습 완료시 모델을 파일로 저장\n","    if (save):\n","      self.save()\n","  \n","\n","  # Y 하나에 대해 예측 수행\n","  def y_predict(self, X_test, y_idx):\n","    # 학습 완료된 모델 갯수 확인\n","    size = len(self.trained_models[y_idx])\n","    # 예측 결과를 넣을 ndarray 생성\n","    test_pred = np.zeros((X_test.shape[0], size))\n","\n","    # 학습 완료된 모델 갯수만큼 예측 수행 \n","    for counter in range(size):\n","      test_pred[:, counter] = self.trained_models[y_idx][counter].predict(X_test)\n","\n","    # 예측된 결과값에 대해 평균을 구함\n","    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1)\n","\n","    # 예측된 결과값을 class 내 저장\n","    self.test_preds[y_idx] = test_pred_mean\n","\n","    return test_pred_mean\n","  \n","\n","  # 머신러닝 훈련 실행\n","  # 파라미터 직접 적용시 동일 파라미터가 전체 적용\n","  # 불러오기 적용시 개별 적용\n","  def fit(self, n_folds, use_params = False, params = None, save = False):\n","    print(self.name, 'Started')\n","\n","    # use_params이 참이고, params가 없을 시 class 이름으로 하이퍼파라미터 튜닝된 파라미터 불러오기\n","    if (use_params):\n","      if (params is None):\n","        params = self.load_params(self.name)\n","        print('Params Loaded!')\n","        for y_idx in range(14):\n","          print(y_idx, end= ' ')\n","          self.y_fit(n_folds, y_idx, None, params[y_idx], save)\n","\n","      # use_params이 참이고, params이 존재할 시 모델 전체에 동일 params 적용\n","      else:\n","        for y_idx in range(14):\n","          print(y_idx, end= ' ')\n","          self.y_fit(n_folds, y_idx, None, params, save)\n","    \n","    # 파라미터 미적용\n","    else:\n","      for y_idx in range(14):\n","        print(y_idx, end= ' ')\n","        self.y_fit(n_folds, y_idx, save = save)\n","    \n","    print(self.name, 'Trained!')\n","\n","\n","  # 머신러닝 훈련 실행\n","  # 모델 저장시 하나의 모델로 저장하여 모델 경량화\n","  def slim_fit(self, n_folds, use_params = False, params = None, save = False):\n","    print(self.name, 'Started')\n","\n","    # use_params이 참이고, params가 없을 시 class 이름으로 하이퍼파라미터 튜닝된 파라미터 불러오기\n","    if (use_params):\n","      if (params is None):\n","        params = self.load_params(self.name)\n","        print('Params Loaded!')\n","        for y_idx in range(14):\n","          print(y_idx, end= ' ')\n","          self.slim_y_fit(n_folds, y_idx, None, params[y_idx], save)\n","\n","      # use_params이 참이고, params이 존재할 시 모델 전체에 동일 params 적용\n","      else:\n","        for y_idx in range(14):\n","          print(y_idx, end= ' ')\n","          self.slim_y_fit(n_folds, y_idx, None, params, save)\n","    \n","    # 파라미터 미적용\n","    else:\n","      for y_idx in range(14):\n","        print(y_idx, end= ' ')\n","        self.slim_y_fit(n_folds, y_idx, save = save)\n","    \n","    print(self.name, 'Slim!')\n","\n","\n","  # 학습된 모델을 통해 예측\n","  def predict(self, X_test):\n","    for y_idx in range(14):\n","      self.y_predict(X_test, y_idx)\n","\n","    return np.hstack(self.test_preds)\n","\n","\n","  # 예측된 Y_train 결과 반환\n","  def get_train_preds(self):\n","    return np.hstack(self.train_preds)\n","\n","\n","  # 예측된 Y_test 결과 반환\n","  def get_test_preds(self):\n","    return np.hstack(self.test_preds)\n","\n","\n","  # nrmse 값 반환\n","  def nrmse(self, y_idx):\n","    rmse = mean_squared_error(self.Y_train[:,y_idx], self.train_preds[y_idx], squared=False)\n","    nrmse = rmse/np.mean(np.abs(self.Y_train[:,y_idx]))\n","    return nrmse\n","\n","\n","  # 모델의 Cross-validation 점수 반환\n","  def score(self):\n","    all_nrmse = [None] * 14\n","    for y_idx in range(14):\n","      all_nrmse[y_idx] = self.nrmse(y_idx)\n","\n","    score = 1.2 * np.sum(all_nrmse[:7]) + 1.0 * np.sum(all_nrmse[7:14])\n","    return score\n","  \n","\n","  # 모델을 현재 이름으로 저장\n","  def save(self):\n","    saved_data = self.model, self.name, self.train_preds, self.trained_models, self.test_preds\n","    try:\n","      os.mkdir(DATA_PATH + \"ML_model\")\n","    except:\n","      pass\n","    joblib.dump(saved_data, DATA_PATH + \"ML_model/saved_\" + self.name + \".pkl\")\n","    # print(self.name, 'Saved!')\n","\n","\n","  # 모델을 현재 이름 혹은 입력한 이름으로 불러오기\n","  def load(self, name = None):\n","    if (not name):\n","      name = self.name\n","    try:\n","      loaded_data = joblib.load(DATA_PATH + \"ML_model/saved_\" + name + \".pkl\")\n","      self.model, self.name, self.train_preds, self.trained_models, self.test_preds = loaded_data\n","      print(self.name, 'Loaded!')\n","\n","      # 학습 전체가 완료된 모델일시 참 반환, 학습이 남은 모델일시 거짓 반환\n","      if (self.trained_models[13] is None):\n","        return False\n","      else:\n","        return True\n","    except:\n","      print(self.name, 'didnt loaded')\n","      return False\n","\n","\n","  # 파라미터 불러오기\n","  def load_params(self, name):\n","    params = [None] * 14\n","    for y_idx in range(14):\n","      try:\n","        load_study = joblib.load(DATA_PATH + \"tune_param/\" + name + \"/tune_\" + str(y_idx) + \".pkl\")\n","        params[y_idx] = load_study.best_trial.params\n","      except:\n","        print(name, y_idx,'params didnt loaded')\n","\n","    return params"]},{"cell_type":"markdown","metadata":{"id":"q1NtAfdRCI1a"},"source":["## 4.2 CV_ml_model을 확장한 Stacking ensemble을 위한 Meta Learning class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_foR0RCqsioD"},"outputs":[],"source":["class meta_ml_model(CV_ml_model):\n","  def __init__(self, model, X_train, Y_train):\n","    super().__init__(model, X_train, Y_train)\n","    self.ml_models = []\n","    self.trained = []\n","\n","  \n","  # 머신러닝 모델 추가\n","  # 훈련된 CV_ml_model을 받음\n","  def add_trained_ml_models(self, *models:CV_ml_model):\n","    for model in models:\n","      self.ml_models.append(model)\n","      self.trained.append(True)\n","\n","\n","  # 머신러닝 모델 추가\n","  # 훈련안된 머신러닝 모델 함수를 받음\n","  def add_new_ml_models(self, *models):\n","    for model in models:\n","      new_model = CV_ml_model(model, self.X_train, self.Y_train)\n","      self.ml_models.append(new_model)\n","      self.trained.append(False)\n","\n","\n","  # 머신러닝 모델 전체 훈련\n","  # 훈련되어있지 않은 모델만 훈련함\n","  def fit_ml_model(self, n_folds, save = False):\n","    for idx, model in enumerate(self.ml_models):\n","      if (not self.trained[idx]):\n","        model.fit(n_folds, save=save)\n","        self.trained[idx] = True\n","\n","\n","  # class 내에 추가된 머신러닝 모델 전체 불러오기\n","  def load_all_models(self):\n","    for idx, model in enumerate(self.ml_models):\n","      self.trained[idx] = model.load()\n","\n","\n","  # class 내에 추가된 머신러닝 모델 전체 저장\n","  def save_all_models(self):\n","    for model in self.ml_models:\n","      model.save()\n","\n","\n","  # 훈련 여부 수동 체크\n","  def trained_check(self, idx):\n","    self.trained[idx] = True\n","\n","\n","  # 머신러닝 모델 하나 훈련 (파라미터 입력 가능)\n","  def fit_one_ml_model(self, n_folds, model_idx, use_params = False, params = None, save=False):\n","    self.ml_models[model_idx].fit(n_folds, use_params, params, save)\n","    self.trained[model_idx] = True\n","\n","\n","  # 머신러닝 모델 하나 경량 훈련 (파라미터 입력 가능)\n","  def slim_fit_one_ml_model(self, n_folds, model_idx, use_params = False, params = None, save=False):\n","    self.ml_models[model_idx].slim_fit(n_folds, use_params, params, save)\n","    self.trained[model_idx] = True\n","\n","  \n","  # 모델 전체 입력값 예측\n","  def predict_ml_model(self, X_test):\n","    for model in self.ml_models:\n","      model.predict(X_test)\n","\n","  \n","  # 모델 이름 변경\n","  def set_ml_name(self, model_idx, name):\n","    self.ml_models[model_idx].set_name(name)\n","\n","\n","  # 메타 훈련을 위해 해당 Y인덱스만 남기고 훈련\n","  # 훈련되지 않은 모델은 훈련 진행\n","  def meta_fit(self, n_folds):\n","    Y_preds = []\n","\n","    # X_train, Y_train을 통해 학습된 Y_pred를 Y_preds에 추가\n","    for idx, model in enumerate(self.ml_models):\n","      if (not self.trained[idx]):\n","        model.fit(n_folds)\n","        self.trained[idx] = True\n","      Y_preds.append(model.get_train_preds())\n","\n","    Y_preds = np.hstack(Y_preds)\n","\n","    for y_idx in range(14):\n","      print(y_idx, end= ' ')\n","      # y_idx와 동일한 y_pred만을 메타 러닝\n","      X_meta_train = Y_preds[:, [i for i in range(Y_preds.shape[1]) if i % 14 == y_idx]]\n","      super().y_fit(n_folds, y_idx, X_meta_train)\n","\n","    print('Meta', self.name, 'Trained!')\n","\n","\n","  # 학습된 모델을 통해 메타 예측\n","  # meta_fit 선행 필수\n","  def meta_predict(self, X_test):\n","    Y_preds = []\n","    X_test = super().to_np(X_test)\n","\n","    # X_test를 통해 예측된 Y_pred를 Y_preds에 추가\n","    for idx, model in enumerate(self.ml_models):\n","      print(model.name, 'Predict Started!')\n","      model.predict(X_test)\n","      Y_preds.append(model.get_test_preds())\n","\n","    Y_preds = np.hstack(Y_preds)\n","\n","    print('Meta Train Started!')\n","\n","    for y_idx in range(14):\n","      # y_idx와 동일한 y_pred만을 통해 예측\n","      X_meta_test = Y_preds[:, [i for i in range(Y_preds.shape[1]) if i % 14 == y_idx]]\n","      super().y_predict(X_meta_test, y_idx)\n","\n","    return np.hstack(self.test_preds)\n","  \n","  \n","  # 전체 모델에 대한 점수 확인\n","  def scores(self):\n","    scores = {}\n","    for model in self.ml_models:\n","      scores[model.name] = model.score()\n","    return scores"]},{"cell_type":"markdown","metadata":{"id":"grvYT54mP4V9"},"source":["## 4.3 Stacking ensemble 수행"]},{"cell_type":"markdown","metadata":{"id":"mIo6G6mJCRZP"},"source":["### 4.3.1 메타 모델 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"laWDIpak3s2Y"},"outputs":[],"source":["# 메타 모델 생성\n","meta_model = meta_ml_model(Ridge, X_train, Y_train)\n","meta_model.set_name('Meta_Ridge')"]},{"cell_type":"markdown","metadata":{"id":"7lbRgEdDCpr-"},"source":["### 4.3.2 메타 모델 내에 기반 모델 추가"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tbOB5wqd36F5"},"outputs":[],"source":["# 메타모델 내에 튜닝되지 않은 기반 모델 추가\n","meta_model.add_new_ml_models(\n","  LinearRegression, Ridge, Lasso, ElasticNet, LassoLars,\n","  OrthogonalMatchingPursuit, BayesianRidge, ARDRegression, GradientBoostingRegressor, \n","  HistGradientBoostingRegressor, XGBRegressor, LGBMRegressor, CatBoostRegressor\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y005gE0x72xk"},"outputs":[],"source":["# 메타모델 내에 하이퍼파라미터 튜닝된 기반 모델 추가\n","meta_model.add_new_ml_models(\n","  HistGradientBoostingRegressor, XGBRegressor, LGBMRegressor, CatBoostRegressor\n","    )\n","\n","meta_model.set_ml_name(13, 'HistGradientBoostingRegressor_tune')\n","meta_model.set_ml_name(14, 'XGBRegressor_tune')\n","meta_model.set_ml_name(15, 'LGBMRegressor_tune')\n","meta_model.set_ml_name(16, 'CatBoostRegressor_tune')"]},{"cell_type":"markdown","metadata":{"id":"x9yjrS4XP80V"},"source":["### 4.3.3 학습된 모델을 불러오거나 직접 학습"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17911,"status":"ok","timestamp":1661925659336,"user":{"displayName":"Minseok Kim","userId":"11245162585726491253"},"user_tz":-540},"id":"FVq9v1uaAcID","outputId":"fe92b14e-d2c5-42ff-8f3f-064bc2e51d00"},"outputs":[{"name":"stdout","output_type":"stream","text":["LinearRegression Loaded!\n","Ridge Loaded!\n","Lasso Loaded!\n","ElasticNet Loaded!\n","LassoLars Loaded!\n","OrthogonalMatchingPursuit Loaded!\n","BayesianRidge Loaded!\n","ARDRegression Loaded!\n","GradientBoostingRegressor Loaded!\n","HistGradientBoostingRegressor Loaded!\n","XGBRegressor Loaded!\n","LGBMRegressor Loaded!\n","CatBoostRegressor Loaded!\n","HistGradientBoostingRegressor_tune Loaded!\n","XGBRegressor_tune Loaded!\n","LGBMRegressor_tune Loaded!\n","CatBoostRegressor_tune Loaded!\n","Meta_Ridge Loaded!\n"]},{"data":{"text/plain":["True"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# 학습된 모델 불러오기\n","meta_model.load_all_models()\n","meta_model.load()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50307,"status":"ok","timestamp":1661920005465,"user":{"displayName":"Minseok Kim","userId":"11245162585726491253"},"user_tz":-540},"id":"-JDUcXRgMNWy","outputId":"a4f6ad3b-5af8-4e4a-ce65-28bed84dc8f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["GradientBoostingRegressor Started\n","0 1 2 3 4 5 6 7 8 9 10 11 12 13 GradientBoostingRegressor Slim!\n","HistGradientBoostingRegressor Started\n","0 1 2 3 4 5 6 7 8 9 10 11 12 13 HistGradientBoostingRegressor Slim!\n","XGBRegressor Started\n","0 1 2 3 4 5 6 7 8 9 10 11 12 13 XGBRegressor Slim!\n","LGBMRegressor Started\n","0 1 2 3 4 5 6 7 8 9 10 11 12 13 LGBMRegressor Slim!\n","CatBoostRegressor Started\n","0 1 2 3 4 5 6 7 8 9 10 11 12 13 CatBoostRegressor Slim!\n","HistGradientBoostingRegressor_tune Started\n","Params Loaded!\n","0 1 2 3 4 5 6 7 8 9 10 11 12 13 HistGradientBoostingRegressor_tune Slim!\n","XGBRegressor_tune Started\n","Params Loaded!\n","0 1 2 3 4 5 6 7 8 9 10 11 12 13 XGBRegressor_tune Slim!\n","LGBMRegressor_tune Started\n","Params Loaded!\n","0 1 2 3 4 5 6 7 8 9 10 11 12 13 LGBMRegressor_tune Slim!\n","CatBoostRegressor_tune Started\n","Params Loaded!\n","0 1 2 3 4 5 6 7 8 9 10 11 12 13 CatBoostRegressor_tune Slim!\n","0 1 2 3 4 5 6 7 8 9 10 11 12 13 Meta Meta_Ridge Trained!\n"]}],"source":["# 학습된 모델을 불러오지 않고 직접 학습하기\n","\n","# GradientBoostingRegressor에 대해 경량화 훈련\n","meta_model.slim_fit_one_ml_model(10, 8)\n","\n","# HistGradientBoostingRegressor에 대해 경량화 훈련\n","meta_model.slim_fit_one_ml_model(10, 9)\n","\n","# XGBRegressor에 대해 경량화 훈련\n","meta_model.slim_fit_one_ml_model(10, 10, True, {'objective': 'reg:squarederror', 'random_state': SEED})\n","\n","# LGBMRegressor에 대해 경량화 훈련\n","meta_model.slim_fit_one_ml_model(10, 11, True, {'random_state': SEED, 'verbose': -1, 'device': 'gpu'})\n","\n","# CatBoostRegressor에 대해 경량화 훈련\n","meta_model.slim_fit_one_ml_model(10, 12, True, {'loss_function': 'RMSE', 'logging_level': 'Silent', 'random_state': SEED})\n","\n","# HistGradientBoostingRegressor_tune에 대해 하이퍼파라미터 튜닝된 경량화 훈련\n","meta_model.slim_fit_one_ml_model(10, 13, True)\n","\n","# XGBRegressor_tune에 대해 하이퍼파라미터 튜닝된 경량화 훈련\n","meta_model.slim_fit_one_ml_model(10, 14, True)\n","\n","# LGBMRegressor_tune에 대해 하이퍼파라미터 튜닝된 경량화 훈련\n","meta_model.slim_fit_one_ml_model(5, 15, True)\n","\n","# CatBoostRegressor_tune에 대해 하이퍼파라미터 튜닝된 경량화 훈련\n","meta_model.slim_fit_one_ml_model(10, 16, True)\n","\n","# 학습 안된 기반 모델 학습\n","meta_model.fit_ml_model(10)\n","\n","# 기반 모델의 예측 결과를 최종 데이터 세트로 하여 메타 모델 학습\n","meta_model.meta_fit(10)"]},{"cell_type":"markdown","metadata":{"id":"s5jxVq1BDI4E"},"source":["# 5 성능 평가"]},{"cell_type":"markdown","metadata":{"id":"3kFxpd1xGBTH"},"source":["메타 모델 내의 기반 모델 별 성능 평가 (Cross-validation)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":490,"status":"ok","timestamp":1661925659806,"user":{"displayName":"Minseok Kim","userId":"11245162585726491253"},"user_tz":-540},"id":"USXfkfhbWAS8","outputId":"f5cabbe9-edb8-458f-ba14-7e9c5cb60fd5"},"outputs":[{"data":{"text/plain":["{'LinearRegression': 1.9832964715433596,\n"," 'Ridge': 1.98364582142708,\n"," 'Lasso': 2.011257876889375,\n"," 'ElasticNet': 2.0111884571234477,\n"," 'LassoLars': 2.012444170863825,\n"," 'OrthogonalMatchingPursuit': 1.995407540468963,\n"," 'BayesianRidge': 1.9836601168170116,\n"," 'ARDRegression': 1.9866843993140668,\n"," 'GradientBoostingRegressor': 1.9616975573307935,\n"," 'HistGradientBoostingRegressor': 1.9503053619519914,\n"," 'XGBRegressor': 1.9940671623569788,\n"," 'LGBMRegressor': 1.9487536436470618,\n"," 'CatBoostRegressor': 1.9468092240305752,\n"," 'HistGradientBoostingRegressor_tune': 1.9452303685210361,\n"," 'XGBRegressor_tune': 1.9359791002432554,\n"," 'LGBMRegressor_tune': 1.9385102603433542,\n"," 'CatBoostRegressor_tune': 1.9386097463589151}"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["meta_model.scores()"]},{"cell_type":"markdown","metadata":{"id":"dSMA6OSMGN58"},"source":["메타 모델 성능 평가 (Cross-validation)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1661925659806,"user":{"displayName":"Minseok Kim","userId":"11245162585726491253"},"user_tz":-540},"id":"_BUODGtp5Kug","outputId":"796d8cb6-99d7-4238-ce0f-923c910dcf51"},"outputs":[{"data":{"text/plain":["1.9321191235308735"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["meta_model.score()"]},{"cell_type":"markdown","metadata":{"id":"gvDKIEOLBpak"},"source":["# 6 테스트 데이터 예측"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QfhLEztiBn1H"},"outputs":[],"source":["test = pd.read_csv(DATA_PATH + 'test.csv').drop(columns=['ID'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MWp4b3VmCQ0p"},"outputs":[],"source":["# 영향 없는 인자 제거\n","X_test = test.drop(['X_02', 'X_04', 'X_10', 'X_11', 'X_23', 'X_47', 'X_48'], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"98BXIzmqCZjq"},"outputs":[],"source":["# PCA 변환\n","X_test = pca_5.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":138580,"status":"ok","timestamp":1661925799216,"user":{"displayName":"Minseok Kim","userId":"11245162585726491253"},"user_tz":-540},"id":"SkN-Aq9cCe3F","outputId":"b6057d0a-71bc-4d27-b6cb-6e3b96ba524d"},"outputs":[{"name":"stdout","output_type":"stream","text":["LinearRegression Predict Started!\n","Ridge Predict Started!\n","Lasso Predict Started!\n","ElasticNet Predict Started!\n","LassoLars Predict Started!\n","OrthogonalMatchingPursuit Predict Started!\n","BayesianRidge Predict Started!\n","ARDRegression Predict Started!\n","GradientBoostingRegressor Predict Started!\n","HistGradientBoostingRegressor Predict Started!\n","XGBRegressor Predict Started!\n","LGBMRegressor Predict Started!\n","CatBoostRegressor Predict Started!\n","HistGradientBoostingRegressor_tune Predict Started!\n","XGBRegressor_tune Predict Started!\n","LGBMRegressor_tune Predict Started!\n","CatBoostRegressor_tune Predict Started!\n","Meta Train Started!\n","CPU times: user 2min 32s, sys: 3.48 s, total: 2min 35s\n","Wall time: 2min 18s\n"]}],"source":["%%time\n","\n","Y_pred = meta_model.meta_predict(X_test)"]},{"cell_type":"markdown","metadata":{"id":"ht4QiBdO_F3p"},"source":["## 6.1 CSV 파일로 저장"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_AaZdSgastXA"},"outputs":[],"source":["def result(test, name = ''):\n","  submit = pd.read_csv(DATA_PATH +'sample_submission.csv')\n","\n","  for idx, col in enumerate(submit.columns):\n","      if col=='ID':\n","          continue\n","      submit[col] = test[:,idx-1]\n","\n","  submit.to_csv(DATA_PATH + f'submission{name}.csv', index=False)\n","  print('Done.')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1724,"status":"ok","timestamp":1661921965711,"user":{"displayName":"Minseok Kim","userId":"11245162585726491253"},"user_tz":-540},"id":"YUl16ZjW_OcR","outputId":"b5ed9a42-bf8d-4109-e7f0-c0738d05f8ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["Done.\n"]}],"source":["result(Y_pred)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"e2e4a47e36f0811c316f759d1a92830bb55dbd78977c69f453a1e4981941b515"}}},"nbformat":4,"nbformat_minor":0}
