{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import random, os\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import SuccessiveHalvingPruner\n",
    "from xgboost import XGBClassifier\n",
    "import optuna\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler\n",
    "from fancyimpute import IterativeImputer, IterativeSVD\n",
    "from sklearn.metrics import log_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "SEED = 25\n",
    "seed_everything(SEED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/train.csv')\n",
    "test = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "train.drop(columns=[\"PRODUCT_ID\", \"TIMESTAMP\"], inplace=True)\n",
    "test.drop(columns=[\"PRODUCT_ID\", \"TIMESTAMP\"], inplace=True)\n",
    "y = train['Y_Class']\n",
    "\n",
    "num_features = test.select_dtypes(exclude=['object']).columns.to_list()\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#train[num_features] = scaler.fit_transform(train[num_features])\n",
    "#test[num_features] = scaler.transform(test[num_features])\n",
    "\n",
    "qual_col = ['LINE', 'PRODUCT_CODE']\n",
    "\n",
    "for i in qual_col:\n",
    "    le = LabelEncoder()\n",
    "    le = le.fit(train[i])\n",
    "    train[i] = le.transform(train[i])\n",
    "    \n",
    "    for label in np.unique(test[i]): \n",
    "        if label not in le.classes_: \n",
    "            le.classes_ = np.append(le.classes_, label)\n",
    "    test[i] = le.transform(test[i]) \n",
    "\n",
    "X = train\n",
    "X_test = test\n",
    "\n",
    "for col in X:\n",
    "    if X[col].nunique() < 2:\n",
    "        X.drop(columns=col, inplace=True)\n",
    "        X_test.drop(columns=col, inplace=True)\n",
    "\n",
    "dup = ~X.T.duplicated()\n",
    "X = X.loc[:, dup]\n",
    "X_test = X_test.loc[:, dup]\n",
    "\n",
    "num_features = X_test.select_dtypes(exclude=['object']).columns.to_list()\n",
    "\n",
    "imputer = KNNImputer()\n",
    "X[num_features] = imputer.fit_transform(X[num_features])\n",
    "X_test[num_features] = imputer.transform(X_test[num_features])\n",
    "\n",
    "#X.fillna(0, inplace=True)\n",
    "#X_test.fillna(0, inplace=True)\n",
    "\n",
    "X = X.drop(columns=['Y_Class', 'Y_Quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid = train_test_split(X, stratify=y, train_size=0.8, random_state=12)\n",
    "y_train, y_valid = X_train['Y_Class'], X_valid['Y_Class']\n",
    "X_train = X_train.drop(columns=['Y_Class', 'Y_Quality'])\n",
    "X_valid = X_valid.drop(columns=['Y_Class', 'Y_Quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.pruners import SuccessiveHalvingPruner\n",
    "\n",
    "def objective(trial):\n",
    "  param = {'verbosity':1,\n",
    "          'objective':'multi:softmax', #\n",
    "          'max_depth':trial.suggest_int('max_depth',3,30),\n",
    "          'learning_rate':trial.suggest_loguniform('learning_rate',1e-8,1e-2),\n",
    "          'n_estimators':trial.suggest_int('n_estimators',100,3000),\n",
    "          'subsample':trial.suggest_loguniform('subsample',0.7,1),\n",
    "          'min_child_weight': trial.suggest_int('min_child_weight', 1, 300 ),\n",
    "          'alpha': trial.suggest_loguniform( 'alpha', 1e-3, 10.0),\n",
    "          'random_state': 42}\n",
    "  \n",
    "  model = XGBClassifier(tree_method='gpu_hist', **param)\n",
    "  model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=100, verbose=False)\n",
    "  pred = model.predict(np.array(X_valid))\n",
    "  log_score = log_loss(np.array(y_valid), pred)\n",
    "\n",
    "  return log_score\n",
    "\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42), pruner=SuccessiveHalvingPruner())\n",
    "study.optimize(objective, n_trials=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6078431372549019\n",
      "0.7246522540640189\n",
      "0.7605466428995841\n",
      "0.6214790040876997\n",
      "0.6283166109253066\n",
      "0.631983130330784\n",
      "0.6839476969577502\n",
      "0.6314060667001843\n",
      "0.689085424379542\n",
      "0.7015862524785194\n",
      "Mean F1: 0.6680846220078291\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "xgb_clfs = []\n",
    "scores = []\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    #print(train_index)\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.values[train_index], y.values[test_index]\n",
    "    \n",
    "    xgb_params = {'max_depth': 14,\n",
    "                    'min_child_weight': 2,\n",
    "                    'learning_rate': 0.046,\n",
    "                    'subsample': 0.99,\n",
    "                    'colsample_bytree': 0.79}\n",
    "            \n",
    "    model = XGBClassifier(n_estimators=1000, tree_method='gpu_hist', random_state=SEED, **xgb_params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=50, verbose=False)\n",
    "    \n",
    "    pred = model.predict(X_valid)\n",
    "    f1 = f1_score(y_valid, pred, average='macro')\n",
    "    scores.append(f1)\n",
    "    print(f1)\n",
    "    \n",
    "    xgb_clfs.append(model)\n",
    "print('Mean F1:', np.mean(scores))\n",
    "#Mean F1: 0.6877400367735194"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 0, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 0, 0,\n",
       "       2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1,\n",
       "       2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 2, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = []\n",
    "for i in range(10):\n",
    "    xgb_pred = xgb_clfs[i].predict_proba(X_test)\n",
    "    pred = xgb_pred\n",
    "    if i == 0:\n",
    "        preds = pred\n",
    "    else:\n",
    "        preds += pred\n",
    "final_pred = np.argmax(preds, axis=1)\n",
    "\n",
    "submit = pd.read_csv('sample_submission.csv')\n",
    "submit['Y_Class'] = final_pred\n",
    "submit.to_csv('submission.csv', index=False)\n",
    "final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2e4a47e36f0811c316f759d1a92830bb55dbd78977c69f453a1e4981941b515"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
