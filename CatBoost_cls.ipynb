{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/timeseriesAI/tsai/blob/master/tutorial_nbs/10_Time_Series_Classification_and_Regression_with_MiniRocket.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# hyperparameter\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Modeling\n",
    "from sklearn.ensemble import BaggingClassifier, GradientBoostingClassifier, VotingClassifier,RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import Pool,CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, QuantileTransformer, RobustScaler\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from fancyimpute import IterativeImputer, KNN\n",
    "from lightgbm import LGBMRegressor\n",
    "from optuna.pruners import SuccessiveHalvingPruner\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/train_reduce.csv')\n",
    "test = pd.read_csv('dataset/test_reduce.csv')\n",
    "\n",
    "train = train.drop(columns=['TIMESTAMP', 'PRODUCT_ID', 'LINE', 'PRODUCT_CODE'])\n",
    "test = test.drop(columns=['TIMESTAMP', 'PRODUCT_ID', 'LINE', 'PRODUCT_CODE'])\n",
    "\n",
    "cat_features = ['LINE', 'PRODUCT_CODE']\n",
    "num_features = [i for i in test.columns if i not in cat_features]\n",
    "\n",
    "y = train['Y_Quality']\n",
    "y_class = train[\"Y_Class\"]\n",
    "\n",
    "for col in num_features:\n",
    "    train[col] = train[col].fillna(train[col].median())\n",
    "\n",
    "scaler = RobustScaler()\n",
    "#scaler = QuantileTransformer()\n",
    "train[num_features] = scaler.fit_transform(train[num_features])\n",
    "test[num_features] = scaler.transform(test[num_features])\n",
    "\n",
    "use_cat = True\n",
    "if use_cat: \n",
    "    X = train.drop(columns=['Y_Class','Y_Quality'])\n",
    "    X_test = test\n",
    "else: \n",
    "    X = train[num_features]\n",
    "    X_test = test[num_features]\n",
    "    \n",
    "corr = pd.read_csv('correlation/correlation_spearman.csv')\n",
    "# Y_Quality 제거\n",
    "corr = corr.iloc[:-1,:]\n",
    "important = list(corr[abs(corr['correlation'])>=0.01]['feature'])\n",
    "X = X[important]\n",
    "X_test = X_test[important]\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knnclassifier = KNeighborsClassifier(1)\n",
    "knnclassifier.fit(y.to_numpy().reshape(-1,1), y_class.to_numpy().reshape(-1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna를 통한 Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_optimization(trial):\n",
    "    score = []\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 42)\n",
    "\n",
    "    # split 개수 스텝 만큼 train, test 데이터셋을 매번 분할\n",
    "    for train_index, valid_index in kf.split(X, y_class):\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train, y_valid = y.values[train_index], y.values[valid_index]\n",
    "        \n",
    "\n",
    "        params = {\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 100, 200, step=5), \n",
    "            'max_depth': trial.suggest_int('max_depth', 10, 30, step=1, log=False), \n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True), \n",
    "            'n_estimators': trial.suggest_int('n_estimators', 1000, 3000, step=1, log=True), \n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 10, 70, step=1, log=False), \n",
    "            'subsample': trial.suggest_uniform('subsample', 0.2, 0.6), \n",
    "            'random_state': 45\n",
    "        }\n",
    "        model = LGBMRegressor(verbose=-1, **params)\n",
    "        model.fit(X_train, y_train,\n",
    "                eval_set=[(X_valid, y_valid)],\n",
    "            )\n",
    "        lgbm_output = model.predict(X_valid)\n",
    "        score.append(mean_squared_error(y_valid, lgbm_output) ** 0.5)\n",
    "    return np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed=42)\n",
    "optim = optuna.create_study(\n",
    "    study_name=\"LGBM_parameter_opt\",\n",
    "    direction=\"minimize\",\n",
    "    sampler=sampler,\n",
    "    pruner=SuccessiveHalvingPruner()\n",
    ")\n",
    "optim.optimize(cb_optimization, n_trials=1000) # 실제 Train에서는 10000~99999 사용 \n",
    "print(\"best nrmse:\", optim.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_macroF1_lgb(truth, predictions):  \n",
    "    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n",
    "    pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n",
    "    f1 = f1_score(truth, pred_labels, average='macro')\n",
    "    return ('macroF1', f1, True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import joblib\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True) #총 6번의 fold 진행\n",
    "n = 0 #x번째 fold인지 기록\n",
    "\n",
    "fold_target_pred = []\n",
    "\n",
    "fold_score = []\n",
    "\n",
    "#파일 디렉토리 생성\n",
    "model_dir = f'./model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "for train_index, valid_index in skf.split(X, y): #label 기준으로 stratified k fold 진행\n",
    "    n += 1\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "    \n",
    "    val_pred_name = [] #validation pred model 이름 저장\n",
    "    val_pred = []      #validation set pred 결과 저장\n",
    "    arg_max_pred = []\n",
    "    target_pred = []   #test set pred 결과 저장\n",
    "    \n",
    "    ### Create Model ###\n",
    "    #CAT model\n",
    "    {'iterations': 144, 'learning_rate': 0.001081938118227713, 'depth': 9, 'od_wait': 186}\n",
    "    start_time_cat = time.time()\n",
    "    model_cat = CatBoostClassifier(verbose = 0,\n",
    "                            learning_rate = 0.02,\n",
    "                            eval_metric=\"TotalF1:average=Macro\",\n",
    "                            random_seed = 42,\n",
    "                            iterations = 5000,\n",
    "                            #ignored_features = [8, 9, 31, 32, 33, 34, 45, 50, 51, 53, 54, 55],\n",
    "                            od_wait = 200,\n",
    "                            use_best_model=True,\n",
    "                            depth = 9)\n",
    "    \n",
    "    model_cat.fit(X_train, y_train, \n",
    "                  eval_set=(X_valid, y_valid))\n",
    "    end_time_cat = time.time()\n",
    "    \n",
    "    \n",
    "    #model cat 저장\n",
    "    cat_path = './model/cat_{}'.format(n)\n",
    "    model_cat.save_model(cat_path)\n",
    "    \n",
    "    #model cat 불러오기\n",
    "    #model_cat.load_model(cat_path)\n",
    "    output = model_cat.predict_proba(X_valid)\n",
    "    val_pred_name.append(\"CatBoostClassifier\")  # 모델 이름 저장\n",
    "    val_pred.append(output)   # validation set pred 결과 저장\n",
    "    arg_max_pred.append(np.argmax(output, axis=1))\n",
    "    \n",
    "    target_pred.append(model_cat.predict_proba(X_test)) # test set pred 결과 저장\n",
    "    \n",
    "    ### LGBM model\n",
    "    start_time_lgb = time.time()\n",
    "    lgbmparams = {'num_leaves': 17,\n",
    "                'max_depth': 11,\n",
    "                'learning_rate': 0.07028290319049474,\n",
    "                'n_estimators': 78,\n",
    "                'class_weight': 'balanced',\n",
    "                'min_child_samples': 12,\n",
    "                'subsample': 0.831632859850219,\n",
    "                'colsample_bytree': 0.9362544923583181,\n",
    "                'reg_alpha': 0.01941513921336218,\n",
    "                'reg_lambda': 0.0021722692515700652}\n",
    "    \n",
    "    model_lgbm = LGBMClassifier(n_estimators = 2000, \n",
    "                                learning_rate = 0.01,\n",
    "                                max_depth = 16,\n",
    "                                min_child_samples = 56,\n",
    "                                subsample = 0.4,\n",
    "                                num_leaves = 160,\n",
    "                                random_state = 42,\n",
    "                                n_jobs = 8,\n",
    "                                verbose=-1,\n",
    "                                )\n",
    "\n",
    "    fit_params = dict(\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        eval_metric = evaluate_macroF1_lgb,\n",
    "        )\n",
    "    \n",
    "    model_lgbm.fit(X_train, y_train, **fit_params)\n",
    "    end_time_lgb = time.time()\n",
    "    \n",
    "    output = model_lgbm.predict_proba(X_valid)\n",
    "    val_pred_name.append(\"LGBMClassifier\")  # 모델 이름 저장\n",
    "    val_pred.append(output)   # validation set pred 결과 저장\n",
    "    arg_max_pred.append(np.argmax(output, axis=1))\n",
    "    \n",
    "    target_pred.append(model_lgbm.predict_proba(X_test)) # test set pred 결과 저장\n",
    "    \n",
    "    #model lgbm 저장\n",
    "    lgbm_path = './model/lgbm_{}.pkl'.format(n)\n",
    "    \n",
    "    # save model\n",
    "    joblib.dump(model_lgbm, lgbm_path)\n",
    "    # load model\n",
    "    #gbm_pickle = joblib.load('lgb.pkl')\n",
    "    #model_lgbm.save_model(lgbm_path)\n",
    "    \n",
    "    #model lgbm 불러오기\n",
    "    #model_lgbm.load(lgbm_path)\n",
    "\n",
    "    ### XGB model\n",
    "    start_time_xgb = time.time()\n",
    "    model_xgb = XGBClassifier(n_estimators = 3000,\n",
    "                              random_state = 42,\n",
    "                              eval_metric = evaluate_macroF1_lgb, \n",
    "                              learning_rate=0.006,\n",
    "                              subsample=0.75, \n",
    "                              colsample_bytree = 0.86,\n",
    "                              max_depth=9,\n",
    "                              tree_method='gpu_hist',\n",
    "                              gpu_id = 0)\n",
    "    \n",
    "    model_xgb.fit(X_train, y_train, verbose=0)\n",
    "    end_time_xgb = time.time()\n",
    "    \n",
    "    output = model_xgb.predict_proba(X_valid)\n",
    "    val_pred_name.append(\"XGBClassifier\")  # 모델 이름 저장\n",
    "    val_pred.append(output)   # validation set pred 결과 저장\n",
    "    arg_max_pred.append(np.argmax(output, axis=1))\n",
    "    target_pred.append(model_xgb.predict_proba(X_test)) # test set pred 결과 저장\n",
    "    \n",
    "    #model xgb 저장\n",
    "    xgb_path = './model/xgb_{}.pkl'.format(n)\n",
    "    joblib.dump(model_xgb, xgb_path)\n",
    "    #model_xgb.save(xgb_path)\n",
    "    \n",
    "    #model xgb 불러오기\n",
    "    #model_xgb.load(xgb_path)\n",
    "  \n",
    "    ### average validation pred ###\n",
    "    preds = np.array(val_pred[0])\n",
    "    for i in range(1, len(val_pred)):\n",
    "        preds += val_pred[i]\n",
    "    preds = preds/len(val_pred)\n",
    "    preds = np.argmax(preds,axis=1)\n",
    "\n",
    "    ### average target pred ###\n",
    "    target_preds = target_pred[0]\n",
    "    for i in range(1, len(target_pred)):\n",
    "        target_preds += target_pred[i]\n",
    "    target_preds = target_preds/len(target_pred)\n",
    "    fold_target_pred.append(target_preds) # append final target pred\n",
    "    \n",
    "    print(\"========== fold %d ==========\" %(n))\n",
    "    for i in range(len(val_pred)):\n",
    "        print(\"%s model F1 : %0.4f\" %(val_pred_name[i], f1_score(y_valid, arg_max_pred[i], average=\"macro\")))\n",
    "        \n",
    "    print('CAT 코드 실행 시간: %10ds' % (end_time_cat - start_time_cat))\n",
    "    print('LGB 코드 실행 시간: %10ds' % (end_time_lgb - start_time_lgb))\n",
    "    print('XGB 코드 실행 시간: %10ds' % (end_time_xgb - start_time_xgb))\n",
    "    print(\"average model F1 : %0.4f\" %(f1_score(y_valid, preds, average='macro')))\n",
    "    fold_score.append(f1_score(y_valid, preds, average='macro'))\n",
    "\n",
    "total_score = fold_score[0]\n",
    "for i in range(1, len(fold_score)):\n",
    "    total_score += fold_score[i]\n",
    "    \n",
    "total_score = total_score/len(fold_score)\n",
    "\n",
    "print(\"==============================\")\n",
    "print(\"Model Sum Average F1 %0.4f\" %(total_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = np.array(fold_target_pred[0])\n",
    "\n",
    "for i in range(1, 5):\n",
    "    final_pred += fold_target_pred[i]\n",
    "\n",
    "final_pred = final_pred/5\n",
    "pred = np.argmax(final_pred, axis=1)\n",
    "submit = pd.read_csv(\"baseline_submission.csv\")\n",
    "submit['Y_Class'] = pred\n",
    "submit.to_csv(\"submission.csv\", index=False)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##custom error function\n",
    "def custom_error(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('dataset/train.csv')\n",
    "test = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "train.drop(columns=[\"PRODUCT_ID\", \"TIMESTAMP\", 'LINE', 'PRODUCT_CODE'], inplace=True)\n",
    "test.drop(columns=[\"PRODUCT_ID\", \"TIMESTAMP\", 'LINE', 'PRODUCT_CODE'], inplace=True)\n",
    "y = train['Y_Class']\n",
    "\n",
    "num_features = test.select_dtypes(exclude=['object']).columns.to_list()\n",
    "\n",
    "\n",
    "#for col in num_features:\n",
    "#    train[col] = train[col].fillna(train[col].median())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "#scaler = QuantileTransformer()\n",
    "train[num_features] = scaler.fit_transform(train[num_features])\n",
    "test[num_features] = scaler.transform(test[num_features])\n",
    "\n",
    "X = train.drop(columns=['Y_Class', 'Y_Quality'])\n",
    "X_test = test\n",
    "\n",
    "#from math import *\n",
    "#corr = pd.read_csv('correlation/correlation.csv')\n",
    "# Y_Quality 제거\n",
    "#corr = corr.iloc[:-1,:]\n",
    "#important = list(corr[abs(corr['correlation'])>=0.1]['feature'])\n",
    "#important\n",
    "#X = X[important]\n",
    "#X_test = X_test[important]\n",
    "\n",
    "dup = ~X.T.duplicated()\n",
    "X = X.loc[:, dup]\n",
    "X_test = X_test.loc[:, dup]\n",
    "\n",
    "X_columns = X.columns\n",
    "#num_features = X_test.select_dtypes(exclude=['object']).columns.to_list()\n",
    "#scaler = StandardScaler()\n",
    "#X[num_features] = scaler.fit_transform(X[num_features])\n",
    "#X_test[num_features] = scaler.transform(X_test[num_features])\n",
    "imputer = KNNImputer()\n",
    "X = imputer.fit_transform(X)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== step 1 ===========\n",
      "\n",
      "\n",
      "=========== step 2 ===========\n",
      "\n",
      "\n",
      "=========== step 3 ===========\n",
      "\n",
      "\n",
      "=========== step 4 ===========\n",
      "\n",
      "\n",
      "=========== step 5 ===========\n",
      "\n",
      "각 분할의 정확도 기록 : [0.7663720308608225, 0.6241908575241909, 0.6288019603184586, 0.646969696969697, 0.679968140183194]\n",
      "평균 정확도 : 0.6692605371712725\n"
     ]
    }
   ],
   "source": [
    "accuracy_history = []\n",
    "kf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 42)\n",
    "\n",
    "scores = []\n",
    "models = []\n",
    "\n",
    "# split 개수 스텝 만큼 train, test 데이터셋을 매번 분할\n",
    "idx = 0\n",
    "for train_index, valid_index in kf.split(X, y):\n",
    "    idx += 1\n",
    "    print(f\"\\n=========== step {idx} ===========\\n\")\n",
    "    X_train, X_test = X[train_index], X[valid_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[valid_index]\n",
    "    params = {\n",
    "\n",
    "    }\n",
    "\n",
    "    model = CatBoostClassifier(task_type=\"GPU\",\n",
    "                               eval_metric=\"TotalF1:average=Macro\", \n",
    "                               classes_count=3, \n",
    "                               use_best_model=True,\n",
    "                               random_state=42,\n",
    "                               loss_function='MultiClass',\n",
    "                               **params)\n",
    "    model.fit(X_train, y_train, \n",
    "            eval_set=[(X_test, y_test)], \n",
    "            verbose = 0\n",
    "        )\n",
    "    #model.fit(X_train, y_train) # 모델 학습\n",
    "    models.append(model)\n",
    "    scores.append(model.get_best_score()[\"validation\"][\"TotalF1:average=Macro\"])\n",
    "\n",
    "print(\"각 분할의 정확도 기록 :\", scores)\n",
    "print(\"평균 정확도 :\", np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, Pool, EShapCalcType, EFeaturesSelectionAlgorithm\n",
    "kf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 24)\n",
    "idx = 0\n",
    "for train_index, valid_index in kf.split(X, y):\n",
    "    idx += 1\n",
    "    train_X, test_X = X.iloc[train_index], X.iloc[valid_index]\n",
    "    train_y, test_y = y.iloc[train_index], y.iloc[valid_index]\n",
    "    if idx == 1:\n",
    "        break\n",
    "\n",
    "#train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2)\n",
    "feature_names = ['F{}'.format(i) for i in range(train_X.shape[1])]\n",
    "\n",
    "\n",
    "model = CatBoostClassifier(task_type=\"GPU\",\n",
    "                            eval_metric=\"TotalF1:average=Macro\", \n",
    "                            loss_function='MultiClass',\n",
    "                            verbose = 0,\n",
    "                            learning_rate = 0.02,\n",
    "                            random_seed = 42,\n",
    "                            iterations = 500,\n",
    "                            od_wait = 200,\n",
    "                            use_best_model=True,\n",
    "                            depth = 9)\n",
    "\n",
    "summary = model.select_features(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    eval_set=(test_X, test_y),\n",
    "    features_for_select=X.columns,\n",
    "    num_features_to_select=150,\n",
    "    steps=3,\n",
    "    algorithm=EFeaturesSelectionAlgorithm.RecursiveByShapValues,\n",
    "    shap_calc_type=EShapCalcType.Regular,\n",
    "    train_final_model=True,\n",
    "    logging_level='Silent',\n",
    "    plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, Pool, EShapCalcType, EFeaturesSelectionAlgorithm\n",
    "from collections import defaultdict\n",
    "kf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 42)\n",
    "idx = 0\n",
    "not_imp = []\n",
    "count = defaultdict(int)\n",
    "for train_index, valid_index in kf.split(X, y):\n",
    "    idx += 1\n",
    "    train_X, valid_X = X.iloc[train_index], X.iloc[valid_index]\n",
    "    train_y, valid_y = y.iloc[train_index], y.iloc[valid_index]\n",
    "\n",
    "    model = CatBoostClassifier(task_type=\"GPU\",\n",
    "                                eval_metric=\"TotalF1:average=Macro\", \n",
    "                                loss_function='MultiClass',\n",
    "                                verbose = 0,\n",
    "                                learning_rate = 0.02,\n",
    "                                random_seed = 42,\n",
    "                                iterations = 500,\n",
    "                                od_wait = 200,\n",
    "                                use_best_model=True,\n",
    "                                depth = 9)\n",
    "    model.fit(X, y, eval_set=[(valid_X, valid_y)], verbose = 0)\n",
    "    important = pd.DataFrame({'feature_importance': model.get_feature_importance(Pool(X)), \n",
    "                'feature_names': X.columns}).sort_values(by=['feature_importance'], \n",
    "                                                            ascending=False)\n",
    "    for col in important[important[\"feature_importance\"] == 0]['feature_names']:\n",
    "        count[col] += 1\n",
    "        \n",
    "for key in count.keys():\n",
    "    if count[key] == 5:\n",
    "        not_imp.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "selected_feature_names = {\n",
    "    \"features\": not_imp\n",
    "}\n",
    "with open(\"correlation/drop_features.json\", 'w') as fp:\n",
    "    json.dump(selected_feature_names, fp, indent='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'iterations': 207,\n",
    "    'learning_rate': 0.09355513642675106,\n",
    "    'depth': 10,\n",
    "    'l2_leaf_reg': 0.00018514951960546424,\n",
    "    'bootstrap_type': 'Bayesian',\n",
    "    'random_strength': 1.023676953660676,\n",
    "    'bagging_temperature': 0.06544495923088894,\n",
    "    'od_type': 'IncToDec',\n",
    "    'od_wait': 49\n",
    "}\n",
    "model = CatBoostClassifier(task_type=\"GPU\",\n",
    "                            eval_metric=\"TotalF1:average=Macro\", \n",
    "                            thread_count=4,\n",
    "                            classes_count=3, \n",
    "                            random_state=23,\n",
    "                            loss_function='MultiClass',\n",
    "                            cat_features= cat_features,\n",
    "                            **params)\n",
    "\n",
    "model.fit(X, y, verbose = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important = pd.DataFrame({'feature_importance': model.get_feature_importance(Pool(X, cat_features=cat_features)), \n",
    "              'feature_names': X.columns}).sort_values(by=['feature_importance'], \n",
    "                                                           ascending=False)\n",
    "important.to_csv(\"correlation/important.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "ensemble = EnsembleVoteClassifier(clfs=models, weights=[1]*10,voting='soft', fit_base_estimators=False)\n",
    "ensemble.fit(None,np.array([0,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test predict\n",
    "pred = ensemble.predict(test_x)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submission file 준비\n",
    "submit = pd.read_csv('sample_submission.csv')\n",
    "submit['Y_Class'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e2e4a47e36f0811c316f759d1a92830bb55dbd78977c69f453a1e4981941b515"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
